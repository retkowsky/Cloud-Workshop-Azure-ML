{"cells":[{"cell_type":"markdown","source":["Azure ML & Azure Databricks notebooks by Parashar Shah.\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}},{"cell_type":"markdown","source":["#Model Building"],"metadata":{}},{"cell_type":"code","source":["import os\nimport pprint\nimport numpy as np\n\nfrom pyspark.ml import Pipeline, PipelineModel\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["import azureml.core\n\n# Check core SDK version number\nprint(\"SDK version:\", azureml.core.VERSION)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">SDK version: 1.0.10\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# import the Workspace class and check the azureml SDK version\nfrom azureml.core import Workspace\n\nws = Workspace.from_config()\nprint('Workspace name: ' + ws.name, \n      'Azure region: ' + ws.location, \n      'Resource group: ' + ws.resource_group, sep = '\\n')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Found the config file in: /databricks/driver/aml_config/config.json\nWorkspace name: AMLSworkspace\nAzure region: westeurope\nResource group: resgrpAMLS\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["#get the train and test datasets\ntrain_data_path = \"AdultCensusIncomeTrain\"\ntest_data_path = \"AdultCensusIncomeTest\"\n\ntrain = spark.read.parquet(train_data_path)\ntest = spark.read.parquet(test_data_path)\n\nprint(\"train: ({}, {})\".format(train.count(), len(train.columns)))\nprint(\"test: ({}, {})\".format(test.count(), len(test.columns)))\n\ntrain.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">train: (25967, 15)\ntest: (6594, 15)\nroot\n-- race: string (nullable = true)\n-- capital_gain: integer (nullable = true)\n-- workclass: string (nullable = true)\n-- hours_per_week: integer (nullable = true)\n-- native_country: string (nullable = true)\n-- sex: string (nullable = true)\n-- education: string (nullable = true)\n-- age: integer (nullable = true)\n-- education_num: integer (nullable = true)\n-- marital_status: string (nullable = true)\n-- capital_loss: integer (nullable = true)\n-- occupation: string (nullable = true)\n-- fnlwgt: integer (nullable = true)\n-- relationship: string (nullable = true)\n-- income: string (nullable = true)\n\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["#Define Model"],"metadata":{}},{"cell_type":"code","source":["label = \"income\"\ndtypes = dict(train.dtypes)\ndtypes.pop(label)\n\nsi_xvars = []\nohe_xvars = []\nfeatureCols = []\nfor idx,key in enumerate(dtypes):\n    if dtypes[key] == \"string\":\n        featureCol = \"-\".join([key, \"encoded\"])\n        featureCols.append(featureCol)\n        \n        tmpCol = \"-\".join([key, \"tmp\"])\n        # string-index and one-hot encode the string column\n        #https://spark.apache.org/docs/2.3.0/api/java/org/apache/spark/ml/feature/StringIndexer.html\n        #handleInvalid: Param for how to handle invalid data (unseen labels or NULL values). \n        #Options are 'skip' (filter out rows with invalid data), 'error' (throw an error), \n        #or 'keep' (put invalid data in a special additional bucket, at index numLabels). Default: \"error\"\n        si_xvars.append(StringIndexer(inputCol=key, outputCol=tmpCol, handleInvalid=\"skip\"))\n        ohe_xvars.append(OneHotEncoder(inputCol=tmpCol, outputCol=featureCol))\n    else:\n        featureCols.append(key)\n\n# string-index the label column into a column named \"label\"\nsi_label = StringIndexer(inputCol=label, outputCol='label')\n\n# assemble the encoded feature columns in to a column named \"features\"\nassembler = VectorAssembler(inputCols=featureCols, outputCol=\"features\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["from azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\nimport numpy as np\nimport os\nimport shutil\n\nmodel_name = \"AdultCensus_runHistory.mml\"\nmodel_dbfs = os.path.join(\"/dbfs\", model_name)\nrun_history_name = 'spark-ml-notebook'\n\n# start a training run by defining an experiment\nmyexperiment = Experiment(ws, \"Ignite_AI_Talk\")\nroot_run = myexperiment.start_logging()\n\n# Regularization Rates - \nregs = [0.0001, 0.001, 0.01, 0.1]\n\n# try a bunch of regularization rate in a Logistic Regression model\nfor reg in regs:\n    print(\"Regularization rate: {}\".format(reg))\n    # create a bunch of child runs\n    with root_run.child_run(\"reg-\" + str(reg)) as run:\n        # create a new Logistic Regression model.\n        lr = LogisticRegression(regParam=reg)\n        \n        # put together the pipeline\n        pipe = Pipeline(stages=[*si_xvars, *ohe_xvars, si_label, assembler, lr])\n\n        # train the model\n        model_p = pipe.fit(train)\n        \n        # make prediction\n        pred = model_p.transform(test)\n        \n        # evaluate. note only 2 metrics are supported out of the box by Spark ML.\n        bce = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\n        au_roc = bce.setMetricName('areaUnderROC').evaluate(pred)\n        au_prc = bce.setMetricName('areaUnderPR').evaluate(pred)\n\n        print(\"Area under ROC: {}\".format(au_roc))\n        print(\"Area Under PR: {}\".format(au_prc))\n      \n        # log reg, au_roc, au_prc and feature names in run history\n        run.log(\"reg\", reg)\n        run.log(\"au_roc\", au_roc)\n        run.log(\"au_prc\", au_prc)\n        run.log_list(\"columns\", train.columns)\n\n        # save model\n        model_p.write().overwrite().save(model_name)\n        \n        # upload the serialized model into run history record\n        mdl, ext = model_name.split(\".\")\n        model_zip = mdl + \".zip\"\n        shutil.make_archive(mdl, 'zip', model_dbfs)\n        run.upload_file(\"outputs/\" + model_name, model_zip)        \n        #run.upload_file(\"outputs/\" + model_name, path_or_stream = model_dbfs) #cannot deal with folders\n\n        # now delete the serialized model from local folder since it is already uploaded to run history \n        shutil.rmtree(model_dbfs)\n        os.remove(model_zip)\n        \n# Declare run completed\nroot_run.complete()\nroot_run_id = root_run.id\nprint (\"run id:\", root_run.id)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Regularization rate: 0.0001\nArea under ROC: 0.9097083605819115\nArea Under PR: 0.7632878176480774\nRegularization rate: 0.001\nArea under ROC: 0.9095016333650277\nArea Under PR: 0.7621678874667115\nRegularization rate: 0.01\nArea under ROC: 0.9077819269927244\nArea Under PR: 0.7533201198922834\nRegularization rate: 0.1\nArea under ROC: 0.8990925231800638\nArea Under PR: 0.725793265379943\nrun id: ed9c1448-0f4e-46e0-826c-a26c385130e0\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["#Load all run metrics from run history into a dictionary object.\nchild_runs = {}\n\nfor r in root_run.get_children():\n    child_runs[r.id] = r"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["metrics = root_run.get_metrics(recursive=True)\nbest_run_id = max(metrics, key = lambda k: metrics[k]['au_roc'])\nbest_run = child_runs[best_run_id]\nprint('Best run is:', best_run_id)\nprint('Metrics:', metrics[best_run_id]['au_roc'], metrics[best_run_id]['reg'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Best run is: 6b80c8bb-2de3-470a-8a69-eecaad64cbb6\nMetrics: 0.9097083605819115 0.0001\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["#Download the model from the best run to a local folder\nbest_model_file_name = \"best_model.zip\"\nbest_run.download_file(name = 'outputs/' + model_name, output_file_path = best_model_file_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["#Model Evaluation"],"metadata":{}},{"cell_type":"code","source":["##unzip the model to dbfs (as load() seems to require that) and load it.\nif os.path.isfile(model_dbfs) or os.path.isdir(model_dbfs):\n    shutil.rmtree(model_dbfs)\nshutil.unpack_archive(best_model_file_name, model_dbfs)\n\nmodel_p_best = PipelineModel.load(model_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":["# make prediction\npred = model_p_best.transform(test)\noutput = pred[['hours_per_week','age','marital_status','income','prediction']]\ndisplay(output.limit(8))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>hours_per_week</th><th>age</th><th>marital_status</th><th>income</th><th>prediction</th></tr></thead><tbody><tr><td>10</td><td>60</td><td>Divorced</td><td><=50K</td><td>0.0</td></tr><tr><td>25</td><td>23</td><td>Never-married</td><td><=50K</td><td>0.0</td></tr><tr><td>40</td><td>27</td><td>Married-civ-spouse</td><td><=50K</td><td>0.0</td></tr><tr><td>20</td><td>37</td><td>Divorced</td><td><=50K</td><td>0.0</td></tr><tr><td>40</td><td>53</td><td>Divorced</td><td><=50K</td><td>0.0</td></tr><tr><td>40</td><td>31</td><td>Married-civ-spouse</td><td><=50K</td><td>0.0</td></tr><tr><td>30</td><td>26</td><td>Never-married</td><td><=50K</td><td>0.0</td></tr><tr><td>40</td><td>36</td><td>Married-civ-spouse</td><td><=50K</td><td>0.0</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["# evaluate. note only 2 metrics are supported out of the box by Spark ML.\nbce = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\nau_roc = bce.setMetricName('areaUnderROC').evaluate(pred)\nau_prc = bce.setMetricName('areaUnderPR').evaluate(pred)\n\nprint(\"Area under ROC: {}\".format(au_roc))\nprint(\"Area Under PR: {}\".format(au_prc))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Area under ROC: 0.9097083605819115\nArea Under PR: 0.7632878176480774\n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["#Model Persistence"],"metadata":{}},{"cell_type":"code","source":["##NOTE: by default the model is saved to and loaded from /dbfs/ instead of cwd!\nmodel_p_best.write().overwrite().save(model_name)\nprint(\"saved model to {}\".format(model_dbfs))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">saved model to /dbfs/AdultCensus_runHistory.mml\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["%sh\n\nls -la /dbfs/AdultCensus_runHistory.mml/*"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/dbfs/AdultCensus_runHistory.mml/metadata:\ntotal 0\ndrwxr-xr-x 1 root root   0 Jan 30 13:17 .\ndrwxr-xr-x 1 root root   0 Jan 30 13:17 ..\n-rw-r--r-- 1 root root 736 Jan 30 13:17 part-00000\n-rw-r--r-- 1 root root   0 Jan 30 13:17 _SUCCESS\n\n/dbfs/AdultCensus_runHistory.mml/stages:\ntotal 0\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 .\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 ..\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 00_StringIndexer_e9eac5535e1f\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 01_StringIndexer_8a8109bb0dcd\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 02_StringIndexer_24367c6f8a41\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 03_StringIndexer_61bb60473adb\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 04_StringIndexer_6a560ce46480\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 05_StringIndexer_0b3f84c6b4b6\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 06_StringIndexer_fc8dbc613607\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 07_StringIndexer_bb33dc169270\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 08_OneHotEncoder_5d507dba507c\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 09_OneHotEncoder_b522779dbd5d\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 10_OneHotEncoder_55598cdb23de\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 11_OneHotEncoder_fdf357861c67\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 12_OneHotEncoder_2b5adc556ac5\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 13_OneHotEncoder_3ed900233ad4\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 14_OneHotEncoder_5c94756f39c7\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 15_OneHotEncoder_536a0268d445\ndrwxr-xr-x 1 root root 0 Jan 30 13:17 16_StringIndexer_2d3028e65162\ndrwxr-xr-x 1 root root 0 Jan 30 13:18 17_VectorAssembler_55d435379165\ndrwxr-xr-x 1 root root 0 Jan 30 13:18 18_LogisticRegression_6dad8525703b\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["dbutils.notebook.exit(\"success\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/plain":["success"]}}],"execution_count":20},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":21}],"metadata":{"name":"03.Build_model_runHistory","notebookId":1322211305149453},"nbformat":4,"nbformat_minor":0}
